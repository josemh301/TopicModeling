{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from gensim import corpora\n",
    "from gensim.models import LsiModel\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(path,file_name):\n",
    "\n",
    "    documents_list = []\n",
    "    titles=[]\n",
    "    with open(\"tweets.txt\", mode=\"r\", encoding=\"utf-8\") as fp:\n",
    "        for line in fp.readlines():\n",
    "            text = line.strip()\n",
    "            documents_list.append(text)\n",
    "    print(\"Número total de documentos:\",len(documents_list))\n",
    "    titles.append( text[0:min(len(text),1000)] )\n",
    "    return documents_list,titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add words to stopwords list\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('spanish')\n",
    "with open (\"stopwords.txt\", mode=\"r\", encoding=\"utf-8\") as newSWlist:\n",
    "    for nuevapalabra in newSWlist.readlines():\n",
    "        palabritas = nuevapalabra.strip()\n",
    "        stopwords.append(palabritas)    \n",
    "print (stopwords[-15:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(doc_set):\n",
    "\n",
    "        tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "        sp_stop = stopwords\n",
    "        ps = PorterStemmer()\n",
    "        texts = []\n",
    "\n",
    "        for i in doc_set:\n",
    "            raw = i.lower()\n",
    "            tokens = tokenizer.tokenize(raw)\n",
    "            stopped_tokens = [i for i in tokens if not i in sp_stop]\n",
    "            stemmed_tokens = [ps.stem(i) for i in stopped_tokens]\n",
    "            texts.append(stemmed_tokens)\n",
    "\n",
    "        return texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_corpus(doc_clean):\n",
    "\n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    return dictionary,doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gensim_lsa_model(doc_clean,number_of_topics,words):\n",
    "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
    "    # generate LSA model\n",
    "    lsamodel = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
    "    print(lsamodel.print_topics(num_topics=number_of_topics, num_words=words))\n",
    "    return lsamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, doc_term_matrix, doc_clean, stop, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Input   : dictionary : Gensim dictionary\n",
    "              corpus : Gensim corpus\n",
    "              texts : List of input texts\n",
    "              stop : Max num of topics\n",
    "    purpose : Compute c_v coherence for various number of topics\n",
    "    Output  : model_list : List of LSA topic models\n",
    "              coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, stop, step):\n",
    "        # generate LSA model\n",
    "        model = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(doc_clean,start, stop, step):\n",
    "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
    "    model_list, coherence_values = compute_coherence_values(dictionary, doc_term_matrix,doc_clean,\n",
    "                                                            stop, start, step)\n",
    "    # Show graph\n",
    "    x = range(start, stop, step)\n",
    "    plt.plot(x, coherence_values)\n",
    "    plt.xlabel(\"Number of Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"coherence_values\"), loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSA Model\n",
    "number_of_topics=5\n",
    "words=5\n",
    "document_list,titles=load_data(\"\",\"tweets.txt\")\n",
    "clean_text=preprocess_data(document_list)\n",
    "model=create_gensim_lsa_model(clean_text,number_of_topics,words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start,stop,step=2,12,1\n",
    "plot_graph(clean_text,start,stop,step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#En este apartado vamos a crear una imagen visual que nos indique a través de su tamaño, cuales son las palabras que mas aparecen en nuestro corpus. Esto no tiene unicamente una función decorativa, sino que podemos :\n",
    "\n",
    "    # Import the wordcloud library\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "lista_definitiva = []\n",
    "n = 1\n",
    "while n < number_of_topics:\n",
    "    lista_definitiva.append(clean_text[n])\n",
    "    n =+1\n",
    "print (lista_definitiva)\n",
    "\n",
    "    # Join the different processed titles together.\n",
    "long_string = ','.join(lista_definitiva))\n",
    "\n",
    "    # Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=2, contour_color='steelblue')\n",
    "\n",
    "    # Generate a word cloud\n",
    "wordcloud.generate(long_string)\n",
    "\n",
    "    # Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (clean_text)\n",
    "for y in clean_text:\n",
    "    for x in y:\n",
    "        lista_definitiva.append(x)\n",
    "print (lista_definitiva)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
