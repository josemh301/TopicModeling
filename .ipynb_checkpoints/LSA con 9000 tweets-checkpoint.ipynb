{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from gensim import corpora\n",
    "from gensim.models import LsiModel\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quitar signos de puntuación\n",
    "\n",
    "def RemoveComma(text):\n",
    "    regex= r\"\\,\"\n",
    "    text = re.sub(regex,\"\",text) \n",
    "    return text\n",
    "\n",
    "def RemoveUpComma(text):\n",
    "    regex= r\"\\’\"\n",
    "    text = re.sub(regex,\" \",text)\n",
    "    return text\n",
    "\n",
    "def RemoveDot(text):\n",
    "    regex= r\"\\.\"\n",
    "    text = re.sub(regex,\"\",text)\n",
    "    return text\n",
    "\n",
    "def RemoveColon(text):\n",
    "    regex= r\"\\:\"\n",
    "    text = re.sub(regex,\"\",text)\n",
    "    return text\n",
    "\n",
    "def RemoveQuotMarks(text):\n",
    "    regex= r\"\\\"\"\n",
    "    text = re.sub(regex,\"\",text)\n",
    "    return text\n",
    "\n",
    "def RemoveExclamation1(text):\n",
    "    regex= r\"\\¡\"\n",
    "    text = re.sub(regex,\"\",text)\n",
    "    return text\n",
    "\n",
    "def RemoveExclamation2(text):\n",
    "    regex= r\"\\!\"\n",
    "    text = re.sub(regex,\"\",text)\n",
    "    return text\n",
    "\n",
    "def RemoveExclamation(tweet):\n",
    "    tweet=RemoveExclamation1(tweet)\n",
    "    tweet=RemoveExclamation2(tweet)\n",
    "    return tweet\n",
    "\n",
    "def RemoveQuestion1(text):\n",
    "    regex= r\"\\¿\"\n",
    "    text = re.sub(regex,\"\",text) \n",
    "    return text\n",
    "\n",
    "def RemoveQuestion2(text):\n",
    "    regex= r\"\\?\"\n",
    "    text = re.sub(regex,\"\",text)\n",
    "    return text\n",
    "\n",
    "def RemoveQuestion(tweet):\n",
    "    tweet=RemoveQuestion1(tweet)\n",
    "    tweet=RemoveQuestion2(tweet)\n",
    "    return tweet\n",
    "\n",
    "def RemoveHyphen(text):\n",
    "    regex= r\"\\-\" \n",
    "    text = re.sub(regex,\"\",text)\n",
    "    return text\n",
    "    \n",
    "\n",
    "def RemovePunctuation(tweet):\n",
    "    tweet=RemoveComma(tweet)\n",
    "    tweet=RemoveDot(tweet)\n",
    "    tweet=RemoveQuotMarks(tweet)\n",
    "    tweet=RemoveExclamation(tweet)\n",
    "    tweet=RemoveColon(tweet)\n",
    "    tweet=RemoveUpComma(tweet)\n",
    "    tweet=RemoveQuestion(tweet)\n",
    "    tweet=RemoveHyphen(tweet)\n",
    "    return tweet\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveLinks(text):\n",
    "    regex= r\"http\\S+\"\n",
    "    text = re.sub(regex,\"\",text)\n",
    "    return text\n",
    "\n",
    "def RemoveAt(text):\n",
    "    regex= r\"@\\w+\"\n",
    "    text = re.sub(regex,\"\",text)\n",
    "    return text\n",
    "\n",
    "def RemoveHashtag(text):\n",
    "    regex= r\"#\\w+\" \n",
    "    text = re.sub(regex,\"\",text)\n",
    "    return text\n",
    "\n",
    "def RemoveNLine(text):\n",
    "    regex= r\"\\n\" #ESTA NO FUNCIONA\n",
    "    text = re.sub(regex,\"\",text)\n",
    "    return text\n",
    "\n",
    "def RemoveNoise(tweet):\n",
    "    tweet=RemoveLinks(tweet)\n",
    "    tweet=RemoveAt(tweet)\n",
    "    tweet=RemoveHashtag(tweet)\n",
    "    tweet=RemoveNLine(tweet)\n",
    "    return tweet\n",
    "\n",
    "def RemoveNumbers(text):\n",
    "    regex= r'\\d+'\n",
    "    text = re.sub(regex,\"\",text)\n",
    "    return text\n",
    "\n",
    "def limpiar (tweet): \n",
    "    tweet=RemoveNoise(tweet)\n",
    "    tweet=RemovePunctuation(tweet)\n",
    "    tweet=RemoveNumbers(tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tweet 1 limpito queda asi:   pero si en la aldea buscan a los mismos  para lo mismo lo pensaría Aqui contratan a los mismos influencer como que si la palabra de ellos es ley Espero ver en esa campaña a medicos enfermeras y personas normales \n",
      "El tweet 2 limpito queda asi:   Hui han continuat les vacunacions a l hospital de Gandia \n",
      "El tweet 3 limpito queda asi:  Ministerio de Minería RT  Llegamos a los  vacunados  Seguiremos desplegándonos por todo el territorio nacional combatiendo el Covid  \n",
      "El tweet 4 limpito queda asi:  Ya estamos en Valparaíso con las vacunas para el personal de salud de la región \n",
      "El tweet 5 limpito queda asi:   Ha aceptado a regañadientes  Andadura o anduviese  \n"
     ]
    }
   ],
   "source": [
    "ejemplo2 = ['#YoMeVacuno, pero si en la aldea buscan a los mismos , para \"lo mismo\", lo pensaría. Aqui contratan a los mismos influencer como que si la palabra de ellos es \"ley\". Espero ver en esa campaña a medicos, enfermeras y personas \"normales\" .#yomeentiendo', '#VacunaCOVID19 \\nHui han continuat les vacunacions a l’hospital de Gandia\\n#joemvacune #yomevacuno', 'Ministerio de Minería: RT \\n@GobiernodeChile\\n: ¡Llegamos a los 10.325 vacunados! \\n Seguiremos desplegándonos por todo el territorio nacional combatiendo el Covid-19 \\n #YoMeVacuno', 'Ya estamos en Valparaíso con las vacunas para el personal de salud de la región. #YoMeVacuno', \"@RAEinforma ¿Ha aceptado a regañadientes? #dudaRAE \\n\\n¿Andadura o anduviese? #dudaRAE \\n\\n@RAEinforma\"]\n",
    "n = 0\n",
    "for e in ejemplo2:\n",
    "    n= n+1\n",
    "    print (\"El tweet\", n, \"limpito queda asi: \", limpiar(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path,file_name):\n",
    "\n",
    "    documents_list = []\n",
    "    titles=[]\n",
    "    with open(\"tweets.txt\", mode=\"r\", encoding=\"utf-8\") as fp:\n",
    "        for line in fp.readlines():\n",
    "            text = limpiar(line)\n",
    "            documents_list.append(text)\n",
    "            text = line.strip()\n",
    "    print(\"Número total de documentos:\",len(documents_list))\n",
    "    titles.append( text[0:min(len(text),1000)] )\n",
    "    return documents_list,titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'words'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-7e3a53e66791>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'spanish'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"stopwords.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnewSWlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mnuevapalabra\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnewSWlist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mpalabritas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnuevapalabra\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpalabritas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'words'"
     ]
    }
   ],
   "source": [
    "stopwords = stopwords.words('spanish')\n",
    "with open (\"stopwords.txt\", mode=\"r\", encoding=\"utf-8\") as newSWlist:\n",
    "    for nuevapalabra in newSWlist.readlines():\n",
    "        palabritas = nuevapalabra.strip()\n",
    "        stopwords.append(palabritas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(doc_set):\n",
    "        tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "        texts = []\n",
    "        for i in doc_set:\n",
    "            raw = i.lower()\n",
    "            tokens = tokenizer.tokenize(raw)\n",
    "            stopped_tokens = [i for i in tokens if not i in stopwords]\n",
    "            texts.append(stopped_tokens)\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_corpus(doc_clean):\n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    return dictionary,doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gensim_lsa_model(doc_clean,number_of_topics,words):\n",
    "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
    "    # generate LSA model\n",
    "    lsamodel = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
    "    print(lsamodel.print_topics(num_topics=number_of_topics, num_words=words))\n",
    "    return lsamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, doc_term_matrix, doc_clean, stop, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Input   : dictionary : Gensim dictionary\n",
    "              corpus : Gensim corpus\n",
    "              texts : List of input texts\n",
    "              stop : Max num of topics\n",
    "    purpose : Compute c_v coherence for various number of topics\n",
    "    Output  : model_list : List of LSA topic models\n",
    "              coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, stop, step):\n",
    "        # generate LSA model\n",
    "        model = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(doc_clean,start, stop, step):\n",
    "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
    "    model_list, coherence_values = compute_coherence_values(dictionary, doc_term_matrix,doc_clean,\n",
    "                                                            stop, start, step)\n",
    "    # Show graph\n",
    "    x = range(start, stop, step)\n",
    "    plt.plot(x, coherence_values)\n",
    "    plt.xlabel(\"Number of Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"coherence_values\"), loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSA Model\n",
    "number_of_topics=25\n",
    "words=2\n",
    "document_list,titles=load_data(\"\",\"tweets.txt\")\n",
    "clean_text=preprocess_data(document_list)\n",
    "model=create_gensim_lsa_model(clean_text,number_of_topics,words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start,stop,step=2,10,1\n",
    "plot_graph(clean_text,start,stop,step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for e in clean_text:\n",
    "    for a in e:\n",
    "        tokens.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numero_de_tweets = len(document_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "frecuencias = FreqDist(tokens)\n",
    "frecuencias.plot(15, cumulative = False) #esto saca un grafico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [token for token in tokens if token not in stopwords]\n",
    "Ntokens = len(tokens)\n",
    "Ntipos = len(set(tokens)) #conjunto de los tokens\n",
    "print (Ntokens)\n",
    "print (Ntipos)\n",
    "diversidad = Ntipos/Ntokens\n",
    "print (\"La diversidad léxica del texto es del\", (diversidad*100),\"%\")\n",
    "print (\"El número de palabras con significado por tweet es de\", Ntokens/numero_de_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "    # Join the different processed titles together.\n",
    "long_string = ','.join(list(tokens))\n",
    "    # Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n",
    "    # Generate a word cloud\n",
    "wordcloud.generate(long_string)\n",
    "    # Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
