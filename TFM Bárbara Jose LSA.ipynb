{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFM: versión LSA\n",
    "\n",
    "En esta versión del documento, vamos a tratar de analizar los tweets de la RAE de manera cuantitativa y clasificativa haciendo uso de la herramienta LSA y la librería **Gensim**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing LSA using Gensim\n",
    "---\n",
    "Primero debemos importar todos los módulos que nos van a ser necesarios:\n",
    "``` python \n",
    "\n",
    "    #import modules\n",
    "    import os.path\n",
    "    from gensim import corpora\n",
    "    from gensim.models import LsiModel\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    from gensim.models.coherencemodel import CoherenceModel\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#poner código válido aquí:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "---\n",
    "Ahora creamos una función para cargar el archivo *csv* que nos interesa:\n",
    "``` python \n",
    "\n",
    "def load_data(path,file_name):\n",
    "    \"\"\"\n",
    "    Input  : path and file_name\n",
    "    Purpose: loading text file\n",
    "    Output : list of paragraphs/documents and\n",
    "             title(initial 100 words considred as title of document)\n",
    "    \"\"\"\n",
    "    documents_list = []\n",
    "    titles=[]\n",
    "    with open( os.path.join(path, file_name) ,\"r\") as fin:\n",
    "        for line in fin.readlines():\n",
    "            text = line.strip()\n",
    "            documents_list.append(text)\n",
    "    print(\"Total Number of Documents:\",len(documents_list))\n",
    "    titles.append( text[0:min(len(text),100)] )\n",
    "    return documents_list,titles\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#poner código válido aquí:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing data\n",
    "---\n",
    "El siguiente paso es limpiar los datos. Vamos a tokenizar la parte que nos interesa del documento, vamos a eliminar las *stopwords* y vamos a _perform stemming on text artcle_. Para ello crearemos una nueva función:\n",
    "``` python\n",
    "\n",
    "    def preprocess_data(doc_set):\n",
    "        \n",
    "        \"\"\"\n",
    "        Input  : docuemnt list\n",
    "        Purpose: preprocess text (tokenize, removing stopwords, and stemming)\n",
    "        Output : preprocessed text\n",
    "        \"\"\"\n",
    "        \n",
    "        # initialize regex tokenizer\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        \n",
    "        # create English stop words list\n",
    "        en_stop = set(stopwords.words('english'))\n",
    "        \n",
    "        # Create p_stemmer of class PorterStemmer\n",
    "        p_stemmer = PorterStemmer()\n",
    "        \n",
    "        # list for tokenized documents in loop\n",
    "        texts = []\n",
    "        \n",
    "        # loop through document list\n",
    "        for i in doc_set:\n",
    "            \n",
    "            # clean and tokenize document string\n",
    "            raw = i.lower()\n",
    "            tokens = tokenizer.tokenize(raw)\n",
    "            \n",
    "            # remove stop words from tokens\n",
    "            stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "            \n",
    "            # stem tokens\n",
    "            stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "            \n",
    "            # add tokens to list\n",
    "            texts.append(stemmed_tokens)\n",
    "            \n",
    "        return texts\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#poner código válido aquí:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare corpus\n",
    "---\n",
    "It is necessary to create a document-term matrix and dictionary of terms:\n",
    "```python\n",
    "\n",
    "def prepare_corpus(doc_clean):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input  : clean document\n",
    "    Purpose: create term dictionary of our courpus and Converting list of documents (corpus) into Document Term Matrix\n",
    "    Output : term dictionary and Document Term Matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "    \n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    \n",
    "    # generate LDA model\n",
    "    return dictionary,doc_term_matrix\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#poner código válido aquí:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creacion de modelo LSA con Gensim\n",
    "---\n",
    "Ahora vamos a crear el modelo LSA usando **Gensim**:\n",
    "```python\n",
    "\n",
    "def create_gensim_lsa_model(doc_clean,number_of_topics,words):\n",
    "    \"\"\"\n",
    "    Input  : clean document, number of topics and number of words associated with each topic\n",
    "    Purpose: create LSA model using gensim\n",
    "    Output : return LSA model\n",
    "    \"\"\"\n",
    "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
    "    # generate LSA model\n",
    "    lsamodel = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
    "    print(lsamodel.print_topics(num_topics=number_of_topics, num_words=words))\n",
    "    return lsamodel\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#poner código válido aquí:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinar el número de topics\n",
    "---\n",
    "Veremos a continuación cuantos _topics_ extrae el sistema LSA:\n",
    "```python\n",
    "\n",
    "def compute_coherence_values(dictionary, doc_term_matrix, doc_clean, stop, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Input   : dictionary : Gensim dictionary\n",
    "              corpus : Gensim corpus\n",
    "              texts : List of input texts\n",
    "              stop : Max num of topics\n",
    "    purpose : Compute c_v coherence for various number of topics\n",
    "    Output  : model_list : List of LSA topic models\n",
    "              coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, stop, step):\n",
    "        # generate LSA model\n",
    "        model = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values\n",
    "```\n",
    "\n",
    "---  \n",
    "**Let's plot coherence score values:**\n",
    "```python\n",
    "\n",
    "def plot_graph(doc_clean,start, stop, step):\n",
    "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
    "    model_list, coherence_values = compute_coherence_values(dictionary, doc_term_matrix,doc_clean,\n",
    "                                                            stop, start, step)\n",
    "    # Show graph\n",
    "    x = range(start, stop, step)\n",
    "    plt.plot(x, coherence_values)\n",
    "    plt.xlabel(\"Number of Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"coherence_values\"), loc='best')\n",
    "    plt.show()\n",
    "```\n",
    "---  \n",
    "```python\n",
    "start,stop,step=2,12,1\n",
    "plot_graph(clean_text,start,stop,step)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#poner código válido aquí:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run all the above functions\n",
    "---\n",
    "Primero debemos importar todos los módulos que nos van a ser necesarios:\n",
    "``` python \n",
    "\n",
    "# LSA Model\n",
    "number_of_topics=7\n",
    "words=10\n",
    "document_list,titles=load_data(\"\",\"articles.txt\")\n",
    "clean_text=preprocess_data(document_list)\n",
    "model=create_gensim_lsa_model(clean_text,number_of_topics,words)\n",
    "```\n",
    "---\n",
    "```python\n",
    "Total Number of Documents: 4551\n",
    "[(0, '0.361*\"trump\" + 0.272*\"say\" + 0.233*\"said\" + 0.166*\"would\" + 0.160*\"clinton\" + 0.140*\"peopl\" + 0.136*\"one\" + 0.126*\"campaign\" + 0.123*\"year\" + 0.110*\"time\"'), (1, '-0.389*\"citi\" + -0.370*\"v\" + -0.356*\"h\" + -0.355*\"2016\" + -0.354*\"2017\" + -0.164*\"unit\" + -0.159*\"west\" + -0.157*\"manchest\" + -0.116*\"apr\" + -0.112*\"dec\"'), (2, '0.612*\"trump\" + 0.264*\"clinton\" + -0.261*\"eu\" + -0.148*\"say\" + -0.137*\"would\" + 0.135*\"donald\" + -0.134*\"leav\" + -0.134*\"uk\" + 0.119*\"republican\" + -0.110*\"cameron\"'), (3, '-0.400*\"min\" + 0.261*\"eu\" + -0.183*\"goal\" + -0.152*\"ball\" + -0.132*\"play\" + 0.128*\"said\" + 0.128*\"say\" + -0.126*\"leagu\" + 0.122*\"leav\" + -0.122*\"game\"'), (4, '0.404*\"bank\" + -0.305*\"eu\" + -0.290*\"min\" + 0.189*\"year\" + -0.164*\"leav\" + -0.153*\"cameron\" + 0.143*\"market\" + 0.140*\"rate\" + -0.139*\"vote\" + -0.133*\"say\"'), (5, '0.310*\"bank\" + -0.307*\"say\" + -0.221*\"peopl\" + 0.203*\"trump\" + 0.166*\"1\" + 0.164*\"min\" + 0.163*\"0\" + 0.152*\"eu\" + 0.152*\"market\" + -0.138*\"like\"'), (6, '0.570*\"say\" + 0.237*\"min\" + -0.170*\"vote\" + 0.158*\"govern\" + -0.154*\"poll\" + 0.122*\"tax\" + 0.115*\"statement\" + 0.115*\"bank\" + 0.112*\"budget\" + -0.108*\"one\"')]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#poner código válido aquí:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
